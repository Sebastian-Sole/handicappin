# Queue-Based Handicap Calculation Implementation Plan

## Overview

Replace the current synchronous trigger-based HTTP webhook architecture with a reliable, user-centric queue-based system that decouples round approval from handicap calculation processing. This eliminates blocking database operations, provides retry mechanisms, improves observability, and removes security vulnerabilities associated with HTTP calls from database triggers.

## Current State Analysis

### Existing Architecture (Problems)

**File**: `supabase/migrations/20251011095000_stripe_functions_and_triggers.sql`

**Current Flow**:

1. Round INSERT/UPDATE/DELETE → Trigger fires (`notify_handicap_engine()`)
2. Trigger makes synchronous HTTP POST to Edge Function via `pg_net`
3. Edge Function calculates handicaps for ALL user's rounds
4. Database transaction waits for HTTP response

**Issues**:

- **Blocking**: Database connection held during entire HTTP call
- **No retry logic**: Failed HTTP calls are lost
- **Security**: Vault secrets complexity, potential SSRF vulnerability
- **Race conditions**: Multiple round approvals trigger duplicate calculations
- **Poor observability**: Debug table is a workaround for lack of visibility
- **Scalability**: Bulk approvals (100+ rounds) cause sequential HTTP calls

### Current Trigger Code Location

- **Trigger function**: Lines 24-170 in `supabase/migrations/20251011095000_stripe_functions_and_triggers.sql`
- **Trigger definition**: Lines 173-177
- **Debug table**: Lines 11-21 (will be removed)

### Existing Edge Function

- **Location**: `supabase/functions/handicap-engine/index.ts`
- **Behavior**: Fetches ALL approved rounds for a user, recalculates all handicaps
- **Will reuse**: Core calculation logic (lines 121-403)

## Desired End State

### New Architecture (Solution)

**User-Centric Queue Model**:

1. Round INSERT/UPDATE/DELETE → Trigger fires (fast, non-blocking)
2. Trigger UPSERTS user to queue table (single entry per user)
3. pg_cron job invokes processor Edge Function every minute
4. Processor fetches up to 25 users, processes in parallel
5. Each user's handicaps calculated once, all rounds updated
6. Successful jobs deleted, failed jobs retained for review

**Key Characteristics**:

- ✅ Queue tracks **users needing recalculation**, not individual rounds
- ✅ Multiple round changes for same user → single queue entry
- ✅ Parallel processing within Edge Function (Promise.allSettled)
- ✅ Configurable batch size via environment variable
- ✅ Automatic retry with failure tracking
- ✅ No HTTP calls from database layer

### Verification Criteria

**Automated Verification**:

- [x] Migration runs successfully: `supabase db reset`
- [x] TypeScript compilation passes: `pnpm build`
- [x] No linting errors: `pnpm lint`
- [x] Queue table created with correct schema
- [x] pg_cron extension enabled
- [ ] Scheduled job created and visible in `cron.job` table (requires manual setup - see migration comments)

**Manual Verification**:

- [ ] Approve a round → User appears in queue within 1 second
- [ ] Approve multiple rounds for same user → Only 1 queue entry
- [ ] Queue processor runs every minute (check logs)
- [ ] Successful calculation → Queue entry deleted
- [ ] Failed calculation → Entry marked with error, attempts incremented
- [ ] After 3 failed attempts → Status set to 'failed'
- [ ] Batch size respects `HANDICAP_QUEUE_BATCH_SIZE` env var
- [ ] Parallel processing completes in <10 seconds for 25 users

## What We're NOT Doing

- NOT implementing optimistic locking (can add later if needed)
- NOT supporting in-flight rounds during migration (clean slate)
- NOT keeping historical debug logs (dropping trigger_debug_log table)
- NOT processing individual rounds separately (user-centric only)
- NOT implementing dead letter queue (failed jobs stay in main queue)
- NOT adding monitoring/alerting (can add later)
- NOT pushing to production database (test locally first)

## Implementation Approach

The migration will happen in distinct phases, each building on the previous:

1. **Infrastructure first**: Create queue table and enable extensions
2. **Replace trigger**: Lightweight enqueue logic replaces HTTP calls
3. **Build processor**: New Edge Function with parallel processing
4. **Schedule processing**: pg_cron invokes processor
5. **Clean up**: Remove old trigger and debug table

Each phase is independently testable and can be rolled back if needed.

---

## Phase 1: Create Queue Infrastructure

### Overview

Create the database table to track which users need handicap recalculation, add Drizzle schema definitions, and enable required PostgreSQL extensions.

### Changes Required

#### 1. Drizzle Schema

**File**: `db/schema.ts`

Add this table definition after the `webhookEvents` table (around line 415):

```typescript
export const handicapCalculationQueue = pgTable(
  "handicap_calculation_queue",
  {
    id: serial("id").primaryKey(),
    userId: uuid("user_id").notNull().unique(),
    eventType: text("event_type").notNull(),
    createdAt: timestamp("created_at")
      .default(sql`CURRENT_TIMESTAMP`)
      .notNull(),
    lastUpdated: timestamp("last_updated")
      .default(sql`CURRENT_TIMESTAMP`)
      .notNull(),
    status: text("status")
      .$type<"pending" | "failed">()
      .default("pending")
      .notNull(),
    attempts: integer("attempts").default(0).notNull(),
    errorMessage: text("error_message"),
  },
  (table) => [
    index("idx_handicap_queue_status_created").on(
      table.status,
      table.createdAt
    ),
    index("idx_handicap_queue_user_id").on(table.userId),
    foreignKey({
      columns: [table.userId],
      foreignColumns: [profile.id],
      name: "handicap_calculation_queue_user_id_fkey",
    })
      .onUpdate("cascade")
      .onDelete("cascade"),
  ]
);

export const handicapCalculationQueueSchema = createSelectSchema(
  handicapCalculationQueue
);
export type HandicapCalculationQueue = InferSelectModel<
  typeof handicapCalculationQueue
>;
```

#### 2. Database Migration

- Should be generated from the schema with `npx drizzle-kit generate`

### Success Criteria

#### Automated Verification:

- [x] Migration file runs without errors: `supabase db reset`
- [x] TypeScript compiles: `pnpm build`

#### Manual Verification:

- [ ] Table exists in database: `SELECT * FROM handicap_calculation_queue;`
- [ ] Extensions enabled: `SELECT * FROM pg_extension WHERE extname IN ('pg_cron', 'pg_net');`
- [ ] Indexes created: `\d handicap_calculation_queue` in psql
- [ ] Foreign key constraint works: Try inserting invalid user_id (should fail)
- [ ] UNIQUE constraint works: Try inserting duplicate user_id (should fail)

---

## Phase 2: Replace Trigger Function

### Overview

Replace the complex HTTP-calling trigger with a simple, fast trigger that only writes to the queue. This removes all vault secret lookups, HTTP calls, and blocking operations from the database layer.

### Changes Required

#### 1. New Trigger Function

**File**: `supabase/migrations/20251207010000_replace_handicap_trigger.sql` (new)

```sql
-- Drop the old HTTP-calling trigger and function
DROP TRIGGER IF EXISTS trigger_handicap_recalculation ON public.round;
DROP FUNCTION IF EXISTS public.notify_handicap_engine();

-- Create new lightweight trigger function that only enqueues users
CREATE OR REPLACE FUNCTION public.enqueue_handicap_calculation()
RETURNS trigger
LANGUAGE plpgsql
SECURITY INVOKER
AS $$
DECLARE
  target_user_id UUID;
  event_type_value TEXT;
BEGIN
  -- Determine user_id and event type based on operation
  IF TG_OP = 'DELETE' THEN
    target_user_id := OLD."userId";
    event_type_value := 'round_delete';
  ELSE
    target_user_id := NEW."userId";
    IF TG_OP = 'INSERT' THEN
      event_type_value := 'round_insert';
    ELSE
      event_type_value := 'round_update';
    END IF;
  END IF;

  -- UPSERT into queue: if user already queued, just update timestamp
  -- This ensures only one entry per user regardless of how many rounds change
  INSERT INTO public.handicap_calculation_queue (
    user_id,
    event_type,
    last_updated
  )
  VALUES (
    target_user_id,
    event_type_value,
    NOW()
  )
  ON CONFLICT (user_id)
  DO UPDATE SET
    last_updated = NOW(),
    event_type = EXCLUDED.event_type;  -- Update to most recent event type

  -- Return appropriate value for trigger
  IF TG_OP = 'DELETE' THEN
    RETURN OLD;
  ELSE
    RETURN NEW;
  END IF;
END;
$$;

-- Create the new trigger on round table
CREATE TRIGGER trigger_handicap_recalculation
  AFTER INSERT OR UPDATE OR DELETE ON public.round
  FOR EACH ROW
  EXECUTE FUNCTION public.enqueue_handicap_calculation();

-- Add comment for documentation
COMMENT ON FUNCTION public.enqueue_handicap_calculation() IS
  'Lightweight trigger that enqueues users for handicap recalculation. Uses UPSERT to ensure only one queue entry per user.';
```

### Success Criteria

#### Automated Verification:

- [x] Migration runs successfully: `supabase db reset`
- [x] Old trigger and function removed
- [x] New trigger and function created

#### Manual Verification:

- [ ] Insert a round → User appears in queue:
  ```sql
  INSERT INTO round (...) VALUES (...);
  SELECT * FROM handicap_calculation_queue WHERE user_id = 'test-user-id';
  ```
- [ ] Insert multiple rounds for same user → Only 1 queue entry:
  ```sql
  INSERT INTO round (...) VALUES (...);
  INSERT INTO round (...) VALUES (...);
  SELECT COUNT(*) FROM handicap_calculation_queue WHERE user_id = 'test-user-id';
  -- Should return 1
  ```
- [ ] Update round → Queue entry updated (check `last_updated`):
  ```sql
  UPDATE round SET notes = 'test' WHERE id = X;
  SELECT last_updated FROM handicap_calculation_queue WHERE user_id = 'test-user-id';
  ```
- [ ] Delete round → User enqueued with 'round_delete' event_type
- [ ] Trigger executes in <10ms (check query logs)

---

## Phase 3: Build Queue Processor Edge Function

### Overview

Create a new Edge Function that processes the queue by fetching pending users, calculating handicaps in parallel, and handling successes/failures appropriately.

### Changes Required

#### 1. Edge Function Implementation

**File**: `supabase/functions/process-handicap-queue/index.ts` (new)

```typescript
import { createClient } from "jsr:@supabase/supabase-js@2";
import "jsr:@supabase/functions-js/edge-runtime.d.ts";

// Import existing handicap calculation logic
import {
  calculateHandicapIndex,
  calculateScoreDifferential,
  calculateCourseHandicap,
  calculateAdjustedGrossScore,
  calculateAdjustedPlayedScore,
  calculateLowHandicapIndex,
  applyHandicapCaps,
  addHcpStrokesToScores,
  ProcessedRound,
} from "../handicap-engine/utils.ts";

import {
  holeResponseSchema,
  roundResponseSchema,
  scoreResponseSchema,
  teeResponseSchema,
} from "../handicap-engine/scorecard.ts";

const EXCEPTIONAL_ROUND_THRESHOLD = 7;
const MAX_SCORE_DIFFERENTIAL = 54;
const ESR_WINDOW_SIZE = 20;

// Configuration from environment variables
const BATCH_SIZE = parseInt(Deno.env.get("HANDICAP_QUEUE_BATCH_SIZE") || "25");
const MAX_RETRIES = parseInt(Deno.env.get("HANDICAP_MAX_RETRIES") || "3");

interface QueueJob {
  id: number;
  user_id: string;
  event_type: string;
  attempts: number;
}

Deno.serve(async (req) => {
  console.log("Queue processor invoked");

  try {
    const supabaseUrl = Deno.env.get("SUPABASE_URL");
    const supabaseServiceRoleKey = Deno.env.get("SUPABASE_SERVICE_ROLE_KEY");

    if (!supabaseUrl || !supabaseServiceRoleKey) {
      throw new Error("Missing Supabase environment variables");
    }

    const supabase = createClient(supabaseUrl, supabaseServiceRoleKey);

    // Fetch pending jobs from queue (up to BATCH_SIZE users)
    const { data: pendingJobs, error: fetchError } = await supabase
      .from("handicap_calculation_queue")
      .select("*")
      .eq("status", "pending")
      .order("created_at", { ascending: true })
      .limit(BATCH_SIZE);

    if (fetchError) {
      throw fetchError;
    }

    if (!pendingJobs || pendingJobs.length === 0) {
      console.log("No pending jobs in queue");
      return new Response(
        JSON.stringify({ message: "No pending jobs", processed: 0 }),
        { status: 200, headers: { "Content-Type": "application/json" } }
      );
    }

    console.log(`Processing ${pendingJobs.length} users from queue`);

    // Process all jobs in parallel using Promise.allSettled
    const results = await Promise.allSettled(
      pendingJobs.map((job: QueueJob) => processUserHandicap(supabase, job))
    );

    // Count successes and failures
    const succeeded = results.filter((r) => r.status === "fulfilled").length;
    const failed = results.filter((r) => r.status === "rejected").length;

    console.log(
      `Queue processing complete: ${succeeded} succeeded, ${failed} failed`
    );

    return new Response(
      JSON.stringify({
        message: "Queue processing complete",
        processed: pendingJobs.length,
        succeeded,
        failed,
      }),
      { status: 200, headers: { "Content-Type": "application/json" } }
    );
  } catch (error: unknown) {
    console.error("Queue processor error:", error);
    const errorMessage =
      error instanceof Error ? error.message : "Unknown error";
    return new Response(JSON.stringify({ error: errorMessage }), {
      status: 500,
      headers: { "Content-Type": "application/json" },
    });
  }
});

/**
 * Process handicap calculation for a single user
 * This reuses the logic from handicap-engine/index.ts
 */
async function processUserHandicap(
  supabase: any,
  job: QueueJob
): Promise<void> {
  try {
    console.log(`Processing user ${job.user_id}, attempt ${job.attempts + 1}`);

    const userId = job.user_id;

    // 1. Fetch user profile
    const { data: userProfile, error: profileError } = await supabase
      .from("profile")
      .select("*")
      .eq("id", userId)
      .single();

    if (profileError || !userProfile) {
      throw new Error(`User profile not found for ${userId}`);
    }

    const initialHandicapIndex =
      userProfile.initialHandicapIndex !== undefined &&
      userProfile.initialHandicapIndex !== null
        ? Number(userProfile.initialHandicapIndex)
        : MAX_SCORE_DIFFERENTIAL;

    // 2. Fetch all approved rounds for user
    const { data: userRoundsRaw, error: roundsError } = await supabase
      .from("round")
      .select("*")
      .eq("userId", userId)
      .eq("approvalStatus", "approved")
      .order("teeTime", { ascending: true });

    if (roundsError) {
      throw roundsError;
    }

    const parsedRounds = roundResponseSchema.safeParse(userRoundsRaw);
    if (!parsedRounds.success) {
      throw new Error("Invalid rounds data: " + parsedRounds.error.message);
    }

    const userRounds = parsedRounds.data;

    // If no approved rounds, set handicap to maximum
    if (!userRounds.length) {
      await supabase
        .from("profile")
        .update({ handicapIndex: MAX_SCORE_DIFFERENTIAL })
        .eq("id", userId);

      // Delete job from queue (success)
      await supabase
        .from("handicap_calculation_queue")
        .delete()
        .eq("id", job.id);

      console.log(`No approved rounds for user ${userId}, handicap set to max`);
      return;
    }

    // 3. Fetch tee info
    const teeIds = new Set(userRounds.map((r) => r.teeId));
    const { data: teesRaw, error: teesError } = await supabase
      .from("teeInfo")
      .select("*")
      .in("id", Array.from(teeIds));

    if (teesError) throw teesError;

    const parsedTees = teeResponseSchema.safeParse(teesRaw);
    if (!parsedTees.success) {
      throw new Error("Invalid tees data: " + parsedTees.error.message);
    }
    const tees = parsedTees.data;

    // 4. Fetch holes
    const { data: holesRaw, error: holesError } = await supabase
      .from("hole")
      .select("*")
      .in("teeId", Array.from(teeIds));

    if (holesError) throw holesError;

    const parsedHoles = holeResponseSchema.safeParse(holesRaw);
    if (!parsedHoles.success) {
      throw new Error("Invalid holes data: " + parsedHoles.error.message);
    }
    const holes = parsedHoles.data;

    // 5. Fetch scores
    const roundIds = userRounds.map((r) => r.id);
    const { data: scoresRaw, error: scoresError } = await supabase
      .from("score")
      .select("*")
      .in("roundId", roundIds);

    if (scoresError) throw scoresError;

    const parsedScores = scoreResponseSchema.safeParse(scoresRaw);
    if (!parsedScores.success) {
      throw new Error("Invalid scores data: " + parsedScores.error.message);
    }
    const scores = parsedScores.data;

    // Build in-memory maps (same as handicap-engine)
    const teeMap = new Map(tees.map((tee) => [tee.id, tee]));
    const roundScoresMap = new Map(
      roundIds.map((roundId) => [
        roundId,
        scores.filter((s) => s.roundId === roundId),
      ])
    );
    const holesMap = new Map(
      Array.from(teeIds).map((teeId) => [
        teeId,
        holes.filter((h) => h.teeId === teeId),
      ])
    );

    // Initialize processed rounds array
    const processedRounds: ProcessedRound[] = userRounds.map((r) => ({
      id: r.id,
      teeTime: new Date(r.teeTime),
      existingHandicapIndex: MAX_SCORE_DIFFERENTIAL,
      rawDifferential: 0,
      esrOffset: 0,
      finalDifferential: 0,
      updatedHandicapIndex: 0,
      adjustedGrossScore: 0,
      adjustedPlayedScore: 0,
      teeId: r.teeId,
      courseHandicap: 0,
      approvalStatus: r.approvalStatus,
    }));

    // Pass 1: Calculate adjusted gross scores
    for (const pr of processedRounds) {
      const teePlayed = teeMap.get(pr.teeId);
      if (!teePlayed) throw new Error(`Tee not found for round ${pr.id}`);

      const roundScores = roundScoresMap.get(pr.id);
      if (!roundScores) throw new Error(`Scores not found for round ${pr.id}`);

      const holes = holesMap.get(pr.teeId);
      if (!holes) throw new Error(`Holes not found for tee ${pr.teeId}`);

      const numberOfHolesPlayed = roundScores.length;

      const courseHandicap = calculateCourseHandicap(
        pr.existingHandicapIndex,
        teePlayed,
        numberOfHolesPlayed
      );

      const scoresWithHcpStrokes = addHcpStrokesToScores(
        holes,
        roundScores,
        courseHandicap,
        numberOfHolesPlayed
      );

      const adjustedPlayedScore = calculateAdjustedPlayedScore(
        holes,
        scoresWithHcpStrokes
      );

      const adjustedGrossScore = calculateAdjustedGrossScore(
        adjustedPlayedScore,
        courseHandicap,
        numberOfHolesPlayed,
        holes,
        roundScores
      );

      pr.adjustedGrossScore = adjustedGrossScore;
      pr.adjustedPlayedScore = adjustedPlayedScore;
      pr.courseHandicap = courseHandicap;
    }

    // Pass 2: Calculate raw differentials and detect ESR
    let rollingIndex = initialHandicapIndex;
    for (let i = 0; i < processedRounds.length; i++) {
      const pr = processedRounds[i];
      pr.existingHandicapIndex = rollingIndex;

      const teePlayed = teeMap.get(pr.teeId);
      if (!teePlayed) throw new Error(`Tee not found for round ${pr.id}`);

      pr.rawDifferential = calculateScoreDifferential(
        pr.adjustedGrossScore,
        teePlayed.courseRating18,
        teePlayed.slopeRating18
      );

      const startIdx = Math.max(0, i - (ESR_WINDOW_SIZE - 1));
      const relevantDifferentials = processedRounds
        .slice(startIdx, i + 1)
        .map((round) => round.rawDifferential);
      pr.updatedHandicapIndex = calculateHandicapIndex(relevantDifferentials);

      const difference = rollingIndex - pr.rawDifferential;
      if (difference >= EXCEPTIONAL_ROUND_THRESHOLD) {
        const offset = difference >= 10 ? 2 : 1;
        const startIdx = Math.max(
          0,
          i - (Math.min(ESR_WINDOW_SIZE, i + 1) - 1)
        );
        for (let j = startIdx; j <= i; j++) {
          processedRounds[j].esrOffset += offset;
        }
      }

      rollingIndex = pr.updatedHandicapIndex;
    }

    // Pass 3: Apply ESR offsets and calculate final differentials
    for (let i = 0; i < processedRounds.length; i++) {
      const pr = processedRounds[i];
      pr.existingHandicapIndex =
        i === 0
          ? initialHandicapIndex
          : processedRounds[i - 1].updatedHandicapIndex;

      pr.finalDifferential = pr.rawDifferential - pr.esrOffset;
      const startIdx = Math.max(0, i - (ESR_WINDOW_SIZE - 1));
      const relevantDifferentials = processedRounds
        .slice(startIdx, i + 1)
        .map((r) => r.finalDifferential);
      const calculatedIndex = calculateHandicapIndex(relevantDifferentials);

      if (processedRounds.length >= 20) {
        const lowHandicapIndex = calculateLowHandicapIndex(processedRounds, i);
        pr.updatedHandicapIndex = applyHandicapCaps(
          calculatedIndex,
          lowHandicapIndex
        );
      } else {
        pr.updatedHandicapIndex = calculatedIndex;
      }

      pr.updatedHandicapIndex = Math.min(
        pr.updatedHandicapIndex,
        MAX_SCORE_DIFFERENTIAL
      );
    }

    // Update all rounds in database
    for (const pr of processedRounds) {
      await supabase
        .from("round")
        .update({
          existingHandicapIndex: pr.existingHandicapIndex,
          scoreDifferential: pr.finalDifferential,
          updatedHandicapIndex: pr.updatedHandicapIndex,
          exceptionalScoreAdjustment: pr.esrOffset,
          adjustedGrossScore: pr.adjustedGrossScore,
          courseHandicap: pr.courseHandicap,
          adjustedPlayedScore: pr.adjustedPlayedScore,
        })
        .eq("id", pr.id);
    }

    // Update user's final handicap index
    await supabase
      .from("profile")
      .update({
        handicapIndex:
          processedRounds[processedRounds.length - 1].updatedHandicapIndex,
      })
      .eq("id", userId);

    // Delete job from queue (success)
    await supabase.from("handicap_calculation_queue").delete().eq("id", job.id);

    console.log(`Successfully processed handicap for user ${userId}`);
  } catch (error: unknown) {
    // Handle failure: update queue entry with error
    console.error(`Failed to process user ${job.user_id}:`, error);

    const errorMessage =
      error instanceof Error ? error.message : "Unknown error";
    const newAttempts = job.attempts + 1;
    const newStatus = newAttempts >= MAX_RETRIES ? "failed" : "pending";

    await supabase
      .from("handicap_calculation_queue")
      .update({
        attempts: newAttempts,
        error_message: errorMessage,
        status: newStatus,
        last_updated: new Date().toISOString(),
      })
      .eq("id", job.id);

    // Re-throw so Promise.allSettled marks as rejected
    throw error;
  }
}
```

#### 2. deno.json Configuration

**File**: `supabase/functions/process-handicap-queue/deno.json` (new)

```json
{
  "imports": {
    "@supabase/supabase-js": "jsr:@supabase/supabase-js@2",
    "@supabase/functions-js": "jsr:@supabase/functions-js"
  }
}
```

#### 3. Environment Variables

**File**: `supabase/.env`

Add these configuration variables:

```bash
# Handicap queue processor configuration
HANDICAP_QUEUE_BATCH_SIZE=25
HANDICAP_MAX_RETRIES=3
```

**For production**, set these via Supabase Dashboard:

- Edge Functions → `process-handicap-queue` → Secrets
- Add `HANDICAP_QUEUE_BATCH_SIZE=25`
- Add `HANDICAP_MAX_RETRIES=3`

### Success Criteria

#### Automated Verification:

- [x] Edge Function created successfully
- [x] TypeScript compiles without errors
- [x] deno.json imports resolve correctly
- [ ] Edge Function deploys successfully: `supabase functions deploy process-handicap-queue` (deployment step pending)

#### Manual Verification:

- [ ] Manually invoke function:
  ```bash
  curl -i --location --request POST \
    'https://PROJECT.supabase.co/functions/v1/process-handicap-queue' \
    --header 'Authorization: Bearer SERVICE_ROLE_KEY' \
    --header 'Content-Type: application/json'
  ```
- [ ] Check function logs: Should show "Queue processor invoked"
- [ ] Add test entry to queue, invoke function → Entry deleted on success
- [ ] Add test entry with invalid user → Entry marked failed after 3 attempts
- [ ] Process 25 users → Completes in <10 seconds (check logs)
- [ ] Verify parallel processing: 25 users with approved rounds all get updated
- [ ] Check error handling: Invalid round data → Queue entry marked with error

---

## Phase 4: Set Up Scheduled Processing

### Overview

Configure pg_cron to automatically invoke the queue processor Edge Function every minute, ensuring continuous background processing of the handicap calculation queue.

### Changes Required

#### 1. Create Cron Job

**File**: `supabase/migrations/20251207020000_schedule_queue_processor.sql` (new)

```sql
-- Schedule the queue processor to run every minute
SELECT cron.schedule(
  'process-handicap-queue',           -- Job name
  '* * * * *',                         -- Every minute (cron syntax)
  $$
  SELECT net.http_post(
    url := 'https://lssnaapatrurmhbbqadb.supabase.co/functions/v1/process-handicap-queue',
    headers := jsonb_build_object(
      'Content-Type', 'application/json',
      'Authorization', 'Bearer ' || current_setting('app.supabase_service_role_key')
    ),
    body := jsonb_build_object(
      'scheduled', true,
      'timestamp', NOW()
    )
  );
  $$
);

-- Set the service role key as a database setting
-- This allows the cron job to authenticate with the Edge Function
-- IMPORTANT: Replace with actual service role key during deployment
ALTER DATABASE postgres SET app.supabase_service_role_key = 'YOUR_SERVICE_ROLE_KEY_HERE';
```

**Note**: For local development, use:

```sql
ALTER DATABASE postgres SET app.supabase_service_role_key = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZS1kZW1vIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImV4cCI6MTk4MzgxMjk5Nn0.EGIM96RAZx35lJzdJsyH-qQwv8Hdp7fsn3W0YpN81IU';
```

**For production**, retrieve the service role key from Supabase Dashboard → Settings → API → service_role key.

### Success Criteria

#### Automated Verification:

- [x] Migration runs successfully: `supabase db reset`
- [x] pg_cron extension enabled
- [ ] Cron job created: `SELECT * FROM cron.job WHERE jobname = 'process-handicap-queue';` (requires manual setup - see migration comments)

#### Manual Verification:

- [ ] Verify cron job configuration:
  ```sql
  SELECT * FROM cron.job WHERE jobname = 'process-handicap-queue';
  -- Should show: schedule='* * * * *', active=true
  ```
- [ ] Wait 1-2 minutes, check job run history:
  ```sql
  SELECT * FROM cron.job_run_details
  WHERE jobid = (SELECT jobid FROM cron.job WHERE jobname = 'process-handicap-queue')
  ORDER BY start_time DESC
  LIMIT 5;
  ```
- [ ] Add entries to queue, wait 1 minute → Entries processed
- [ ] Check Edge Function logs → Should show invocations every minute
- [ ] Verify authentication: Job should succeed with 200 status code
- [ ] Test with empty queue: Should return "No pending jobs" message
- [ ] Test with full batch: 30 users in queue → First 25 processed, remaining 5 in next run

---

## Phase 5: Migration and Cleanup

### Overview

Remove the old trigger, debug table, and any remnants of the HTTP-based architecture. This completes the migration to the queue-based system.

### Changes Required

#### 1. Cleanup Migration

**File**: `supabase/migrations/20251207030000_cleanup_old_trigger.sql` (new)

```sql
-- Remove old debug table (no longer needed)
DROP TABLE IF EXISTS public.trigger_debug_log;

-- Verify old trigger and function are removed (should already be gone from Phase 2)
DROP TRIGGER IF EXISTS trigger_handicap_recalculation ON public.round;
DROP FUNCTION IF EXISTS public.notify_handicap_engine();

-- Add final comment for documentation
COMMENT ON TABLE public.handicap_calculation_queue IS
  'Queue-based handicap calculation system. Users are enqueued when rounds change. Processed every minute by scheduled Edge Function. Replaces legacy HTTP webhook trigger.';

COMMENT ON FUNCTION public.enqueue_handicap_calculation() IS
  'Enqueues users for handicap recalculation when rounds change. Uses UPSERT to coalesce multiple changes per user into single queue entry.';
```

### Success Criteria

#### Automated Verification:

- [x] Migration runs successfully: `supabase db reset`
- [x] Old table removed: `SELECT * FROM trigger_debug_log;` (should error)
- [x] Old function removed: `SELECT * FROM pg_proc WHERE proname = 'notify_handicap_engine';` (returns 0 rows)

#### Manual Verification:

- [ ] Verify database is clean:

  ```sql
  -- Should return 0
  SELECT COUNT(*) FROM pg_tables WHERE tablename = 'trigger_debug_log';

  -- Should return 0
  SELECT COUNT(*) FROM pg_proc WHERE proname = 'notify_handicap_engine';

  -- Should return 1 (new trigger)
  SELECT COUNT(*) FROM pg_proc WHERE proname = 'enqueue_handicap_calculation';
  ```

- [ ] End-to-end test: Submit round → Appears in queue → Processed within 60 seconds → Handicap updated
- [ ] Bulk test: Submit 10 rounds for 5 users → 5 queue entries → All processed in next minute
- [ ] Failure test: Submit round with invalid data → Marked failed after 3 retries, stays in queue
- [ ] Performance test: 25 users with 20 rounds each → All processed in <10 seconds

---

## Testing Strategy

### Unit Tests

**Not required for this migration** - The handicap calculation logic is already tested in the existing `handicap-engine` function. We're only changing the invocation mechanism.

### Integration Tests

Manual integration testing is sufficient for this migration:

1. **Single user, single round**:

   - Insert round → Verify queue entry → Wait 60s → Verify processed

2. **Single user, multiple rounds**:

   - Insert 5 rounds → Verify 1 queue entry → Verify all rounds calculated

3. **Multiple users**:

   - Insert rounds for 10 users → Verify 10 queue entries → All processed

4. **Cascade approval**:

   - Approve course → Verify multiple users queued → All processed

5. **Error handling**:

   - Insert round with invalid teeId → Verify retry → Verify marked failed after 3 attempts

6. **Batch processing**:
   - Queue 30 users → Verify 25 processed in minute 1, 5 in minute 2

### Manual Testing Steps

**After deploying all migrations:**

1. **Reset local database**:

   ```bash
   supabase db reset
   ```

2. **Deploy Edge Function**:

   ```bash
   supabase functions deploy process-handicap-queue
   ```

3. **Set service role key** in migration or manually:

   ```sql
   ALTER DATABASE postgres SET app.supabase_service_role_key = 'YOUR_KEY';
   ```

4. **Test single round submission** via your app UI

5. **Check queue**:

   ```sql
   SELECT * FROM handicap_calculation_queue;
   ```

6. **Wait 60 seconds**

7. **Verify queue is empty** (job processed):

   ```sql
   SELECT * FROM handicap_calculation_queue;
   ```

8. **Verify handicap updated**:

   ```sql
   SELECT "userId", "handicapIndex" FROM profile WHERE id = 'your-test-user';
   ```

9. **Check cron job history**:

   ```sql
   SELECT * FROM cron.job_run_details
   ORDER BY start_time DESC
   LIMIT 5;
   ```

10. **Check Edge Function logs** in Supabase Dashboard

---

## Performance Considerations

### Database Performance

**Queue table**:

- Indexes on `(status, created_at)` and `user_id` ensure fast lookups
- UNIQUE constraint on `user_id` prevents duplicate entries
- ON CONFLICT DO UPDATE ensures fast upserts (~1ms per operation)

**Expected load**:

- Low volume: <100 rounds/day → <100 queue entries/day
- High volume: 1000 rounds/day → Still <1000 queue entries (likely fewer due to user coalescing)
- Queue depth should stay near 0 with minute-based processing

### Edge Function Performance

**Single user calculation**:

- Current handicap-engine takes ~100-200ms per user
- With database query overhead: ~500ms per user

**Batch of 25 users (parallel)**:

- Promise.allSettled processes all in parallel
- Expected completion time: ~2-5 seconds
- Well within 150-second Edge Function timeout

**Cost**:

- 1 invocation/minute = 1,440/day = ~43,200/month
- Free tier: 500,000 invocations/month
- Usage: <10% of free tier

### Scaling Considerations

**If queue depth grows**:

- Increase batch size: `HANDICAP_QUEUE_BATCH_SIZE=50`
- Increase frequency: Change cron to `*/30 * * * *` (every 30 seconds)
- Add optimistic locking (see Phase 2 comments in code)

**Current limits**:

- 25 users/minute = 36,000 users/day maximum throughput
- Far exceeds expected usage for golf handicap application

---

## Migration Notes

### Deployment Order

**For production deployment**:

1. **Deploy migrations in order**:

   ```bash
   supabase db reset --linked
   ```

   This runs all migrations:

   - `20251207000000_create_handicap_queue.sql`
   - `20251207010000_replace_handicap_trigger.sql`
   - `20251207020000_schedule_queue_processor.sql`
   - `20251207030000_cleanup_old_trigger.sql`

2. **Update Drizzle schema**:

   ```bash
   npx drizzle-kit push
   ```

3. **Set service role key**:

   ```sql
   ALTER DATABASE postgres
   SET app.supabase_service_role_key = 'YOUR_ACTUAL_SERVICE_ROLE_KEY';
   ```

4. **Deploy Edge Function**:

   ```bash
   supabase functions deploy process-handicap-queue
   ```

5. **Set Edge Function secrets** in Supabase Dashboard:
   - `HANDICAP_QUEUE_BATCH_SIZE=25`
   - `HANDICAP_MAX_RETRIES=3`

### Rollback Plan

**If issues arise, rollback steps**:

1. **Disable cron job**:

   ```sql
   SELECT cron.unschedule('process-handicap-queue');
   ```

2. **Restore old trigger** (from backup or re-run old migration)

3. **Drop new tables**:

   ```sql
   DROP TABLE IF EXISTS public.handicap_calculation_queue;
   ```

4. **Undeploy Edge Function**:
   ```bash
   supabase functions delete process-handicap-queue
   ```

### Data Migration

**No data migration needed** - This is a clean slate approach:

- Old `trigger_debug_log` data is dropped
- New queue starts empty
- First round submission after migration will populate queue

---

## References

- **Current trigger**: `supabase/migrations/20251011095000_stripe_functions_and_triggers.sql:24-177`
- **Handicap calculation logic**: `supabase/functions/handicap-engine/index.ts:121-403`
- **Database schema**: `db/schema.ts:215-289` (round table)
- **Supabase pg_cron docs**: https://supabase.com/docs/guides/cron
- **Supabase Edge Functions docs**: https://supabase.com/docs/guides/functions
