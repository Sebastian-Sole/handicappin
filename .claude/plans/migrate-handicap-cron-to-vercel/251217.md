# Migrate Handicap Queue Processor from Supabase pg_cron to Vercel Cron

## Overview

Migrate the handicap calculation queue processor from Supabase pg_cron + Edge Function to Vercel cron + Next.js API route. This addresses the fundamental architectural limitation where database-managed secrets (Vault) break during database resets, making the current Supabase solution unsustainable for local development.

## Current State Analysis

**Current Architecture:**
- Database trigger (`enqueue_handicap_calculation`) adds jobs to `handicap_calculation_queue` table
- pg_cron job runs every minute, calls Supabase Edge Function via HTTP
- Edge Function (`process-handicap-queue/index.ts`) processes batch of 25 users
- Uses `handicap-shared/utils.ts` for calculations
- Insecure authentication: checks `body.scheduled === true`

**Key Problems:**
1. **Authentication**: Current check (`body.scheduled === true`) is insecure - anyone can bypass
2. **Vault dependency**: Proper auth would require Supabase Vault, which resets on `supabase db reset`
3. **Manual setup**: Production would require one-time manual vault configuration
4. **Not sustainable**: Breaks local development workflow

**Key Discoveries:**
- Duplicate handicap utilities exist in `utils/calculations/handicap.ts` with bugs
- Edge Function version is source of truth and correct
- Service role key not in validated env schema
- Existing Vercel cron pattern at `app/api/cron/reconcile-stripe/route.ts`

## Desired End State

**New Architecture:**
- Same database trigger (no changes needed)
- Vercel cron job runs every minute, calls Next.js API route
- API route at `app/api/cron/process-handicap-queue/route.ts` processes queue
- Consolidated handicap utilities in `lib/handicap/` (single source of truth)
- Secure authentication via `HANDICAP_CRON_SECRET` header
- Service role key properly validated in env schema

**Verification:**
- Handicap calculations run every minute automatically
- Local development works without manual setup
- Database resets don't break the system
- All environment variables validated by schema
- Edge function completely removed

## What We're NOT Doing

- NOT changing the database trigger or queue table schema
- NOT modifying the RPC functions (`process_handicap_updates`, `process_handicap_no_rounds`)
- NOT removing other edge functions (only `process-handicap-queue`)
- NOT changing the cron frequency (stays at every minute)
- NOT converting to Drizzle ORM (keeping Supabase client for easier migration)
- NOT creating a tRPC endpoint (using Next.js API route for cron)
- NOT changing how handicap calculations work (same algorithm)
- NOT modifying existing Next.js components that use handicap utils (they'll just import from new location)

## Implementation Approach

**Strategy: Incremental migration with zero downtime**

1. **Consolidate utilities first** - Create single source of truth in `lib/handicap/`
2. **Add environment validation** - Add service role key and cron secret to env schema
3. **Create API route** - Build Next.js equivalent of edge function
4. **Update Vercel config** - Schedule new cron job
5. **Test in parallel** - Verify both systems work before switching
6. **Disable old system** - Comment out pg_cron setup
7. **Clean up** - Remove edge function after verification

This approach allows rollback at any step and ensures no lost handicap calculations.

---

## Phase 1: Consolidate Handicap Calculation Utilities

### Overview

Move handicap calculation utilities from `supabase/functions/handicap-shared/` to `lib/handicap/` as the single source of truth. Fix bugs in existing Next.js utilities.

### Changes Required

#### 1. Create `lib/handicap/` Directory Structure

**Files to create:**
- `lib/handicap/calculations.ts` - Pure calculation functions
- `lib/handicap/schemas.ts` - Zod schemas and types
- `lib/handicap/constants.ts` - Shared constants
- `lib/handicap/index.ts` - Barrel exports

#### 2. Port Edge Function Utilities to Next.js

**File**: `lib/handicap/constants.ts`
```typescript
export const EXCEPTIONAL_ROUND_THRESHOLD = 7;
export const MAX_SCORE_DIFFERENTIAL = 54;
export const ESR_WINDOW_SIZE = 20;
export const SOFT_CAP_THRESHOLD = 3.0;
export const HARD_CAP_THRESHOLD = 5.0;
export const LOW_HANDICAP_WINDOW_DAYS = 365;
```

**File**: `lib/handicap/schemas.ts`
**Changes**: Copy from `supabase/functions/handicap-shared/scorecard.ts`
- Change import: `import { z } from "https://esm.sh/zod@3.24.1"` → `import { z } from "zod"`
- Remove `.ts` extensions from imports
- Keep all Zod schemas and inferred types

**File**: `lib/handicap/calculations.ts`
**Changes**: Copy from `supabase/functions/handicap-shared/utils.ts`
- Change imports: `from "./scorecard.ts"` → `from "./schemas"`
- Change imports: Add constant imports `from "./constants"`
- Remove Deno-specific code (none exists)
- Export all functions:
  - `calculateCourseHandicap`
  - `calculateScoreDifferential`
  - `calculateHandicapIndex`
  - `getRelevantDifferentials`
  - `calculateLowHandicapIndex`
  - `applyHandicapCaps`
  - `calculateAdjustedPlayedScore`
  - `calculateHoleAdjustedScore`
  - `calculateAdjustedGrossScore`
  - `addHcpStrokesToScores`
- Export `ProcessedRound` type

**File**: `lib/handicap/index.ts`
```typescript
export * from "./calculations";
export * from "./schemas";
export * from "./constants";
```

#### 3. Update Existing Imports

**Files to update:**
- `utils/calculations/handicap.ts` - Delete file (replaced by lib/handicap)
- `server/api/routers/round.ts` - Change imports to `@/lib/handicap`
- `contexts/roundCalculationContext.tsx` - Change imports to `@/lib/handicap`
- `components/calculators/score-differential.tsx` - Change imports to `@/lib/handicap`

**Import changes:**
```typescript
// OLD:
import { calculateCourseHandicap } from "@/utils/calculations/handicap";

// NEW:
import { calculateCourseHandicap } from "@/lib/handicap";
```

### Success Criteria

#### Automated Verification:
- [x] TypeScript compilation passes: `pnpm build`
- [x] No linting errors: `pnpm lint`
- [ ] All existing tests pass: `pnpm test`
- [x] New utility files exist at correct paths
- [x] Old `utils/calculations/handicap.ts` deleted

#### Manual Verification:
- [ ] Round submission still works correctly
- [ ] Score differential calculator displays correct results
- [ ] No console errors in browser when using calculators
- [ ] Handicap calculations match previous results (spot check)

---

## Phase 2: Add Environment Variable Validation

### Overview

Add `SUPABASE_SERVICE_ROLE_KEY` and `HANDICAP_CRON_SECRET` to the validated environment schema. Rename existing `CRON_SECRET` to `STRIPE_CRON_SECRET` for clarity.

### Changes Required

#### 1. Update Environment Schema

**File**: `env.ts`
**Changes**: Add to `server` section

```typescript
server: {
  // ... existing vars
  SUPABASE_SERVICE_ROLE_KEY: z.string().min(1),
  HANDICAP_CRON_SECRET: z.string().min(1),
  STRIPE_CRON_SECRET: z.string().min(1), // Renamed from CRON_SECRET
  // ... rest of vars
}
```

#### 2. Update Stripe Reconciliation Route

**File**: `app/api/cron/reconcile-stripe/route.ts`
**Line**: 15
**Change**:
```typescript
// OLD:
if (authHeader !== `Bearer ${process.env.CRON_SECRET}`) {

// NEW:
if (authHeader !== `Bearer ${env.STRIPE_CRON_SECRET}`) {
```

Add import at top:
```typescript
import { env } from "@/env";
```

#### 3. Update Environment Files

**File**: `.env.example`
**Changes**: Add new variables and rename existing
```bash
# Old CRON_SECRET removed, add these:
STRIPE_CRON_SECRET=your-stripe-cron-secret
HANDICAP_CRON_SECRET=your-handicap-cron-secret
SUPABASE_SERVICE_ROLE_KEY=your-service-role-key
```

**File**: `.env.local`
**Changes**: Add actual values
```bash
STRIPE_CRON_SECRET=<generate-new-random-string>
HANDICAP_CRON_SECRET=<generate-new-random-string>
SUPABASE_SERVICE_ROLE_KEY=<your-local-service-role-key>
```

#### 4. Update Vercel Environment Variables (Production)

**Manual step**: In Vercel dashboard:
1. Rename `CRON_SECRET` → `STRIPE_CRON_SECRET`
2. Add `HANDICAP_CRON_SECRET` (generate secure random string)
3. Add `SUPABASE_SERVICE_ROLE_KEY` (from Supabase dashboard)

### Success Criteria

#### Automated Verification:
- [ ] Environment validation passes on startup
- [ ] Build succeeds: `pnpm build`
- [ ] Type checking passes: `npx tsc --noEmit`

#### Manual Verification:
- [ ] Stripe reconciliation cron still works with renamed secret
- [ ] No environment validation errors in console
- [ ] Local development starts without errors

---

## Phase 3: Create Handicap Queue Processor API Route

### Overview

Create Next.js API route that replicates the Edge Function logic using Supabase client with service role key.

### Changes Required

#### 1. Create Admin Supabase Client Utility

**File**: `utils/supabase/admin.ts` (new file)
```typescript
import { createClient } from "@supabase/supabase-js";
import { env } from "@/env";
import type { Database } from "@/types/supabase";

/**
 * Creates a Supabase client with service role key for admin operations.
 * WARNING: This bypasses Row Level Security. Only use in secure server contexts.
 */
export function createAdminClient() {
  return createClient<Database>(
    env.NEXT_PUBLIC_SUPABASE_URL,
    env.SUPABASE_SERVICE_ROLE_KEY,
    {
      auth: {
        autoRefreshToken: false,
        persistSession: false,
      },
    }
  );
}
```

#### 2. Create Queue Processor API Route

**File**: `app/api/cron/process-handicap-queue/route.ts` (new file)
**Changes**: Port logic from `supabase/functions/process-handicap-queue/index.ts`

```typescript
import { NextRequest, NextResponse } from "next/server";
import { createAdminClient } from "@/utils/supabase/admin";
import { env } from "@/env";
import {
  calculateHandicapIndex,
  calculateScoreDifferential,
  calculateCourseHandicap,
  calculateAdjustedGrossScore,
  calculateAdjustedPlayedScore,
  calculateLowHandicapIndex,
  applyHandicapCaps,
  addHcpStrokesToScores,
  type ProcessedRound,
  holeResponseSchema,
  roundResponseSchema,
  scoreResponseSchema,
  teeResponseSchema,
  EXCEPTIONAL_ROUND_THRESHOLD,
  MAX_SCORE_DIFFERENTIAL,
  ESR_WINDOW_SIZE,
} from "@/lib/handicap";

const BATCH_SIZE = parseInt(process.env.HANDICAP_QUEUE_BATCH_SIZE || "25");
const MAX_RETRIES = parseInt(process.env.HANDICAP_MAX_RETRIES || "3");

interface QueueJob {
  id: number;
  user_id: string;
  event_type: string;
  attempts: number;
}

/**
 * Handicap Queue Processor
 * Run via Vercel Cron: * * * * * (every minute)
 *
 * Authenticated via HANDICAP_CRON_SECRET environment variable.
 */
export async function GET(request: NextRequest) {
  try {
    // Verify cron secret
    const authHeader = request.headers.get("authorization");
    if (authHeader !== `Bearer ${env.HANDICAP_CRON_SECRET}`) {
      console.warn("Unauthorized access attempt to process-handicap-queue");
      return NextResponse.json({ error: "Unauthorized" }, { status: 403 });
    }

    console.log("Queue processor invoked");

    const supabase = createAdminClient();

    // Fetch pending jobs from queue (up to BATCH_SIZE users)
    const { data: pendingJobs, error: fetchError } = await supabase
      .from("handicap_calculation_queue")
      .select("*")
      .eq("status", "pending")
      .order("created_at", { ascending: true })
      .limit(BATCH_SIZE);

    if (fetchError) {
      throw fetchError;
    }

    if (!pendingJobs || pendingJobs.length === 0) {
      console.log("No pending jobs in queue");
      return NextResponse.json({
        message: "No pending jobs",
        processed: 0,
      });
    }

    console.log(`Processing ${pendingJobs.length} users from queue`);

    // Process all jobs in parallel using Promise.allSettled
    const results = await Promise.allSettled(
      pendingJobs.map((job: QueueJob) => processUserHandicap(supabase, job))
    );

    // Count successes and failures
    const succeeded = results.filter((r) => r.status === "fulfilled").length;
    const failed = results.filter((r) => r.status === "rejected").length;

    console.log(
      `Queue processing complete: ${succeeded} succeeded, ${failed} failed`
    );

    return NextResponse.json({
      message: "Queue processing complete",
      processed: pendingJobs.length,
      succeeded,
      failed,
    });
  } catch (error: unknown) {
    console.error("Queue processor error:", error);
    const errorMessage =
      error instanceof Error ? error.message : "Unknown error";
    return NextResponse.json(
      { error: errorMessage },
      { status: 500 }
    );
  }
}

/**
 * Process handicap calculation for a single user
 * Uses shared utilities from lib/handicap
 */
async function processUserHandicap(
  supabase: any,
  job: QueueJob
): Promise<void> {
  try {
    console.log(`Processing user ${job.user_id}, attempt ${job.attempts + 1}`);

    const userId = job.user_id;

    // 1. Fetch user profile
    const { data: userProfile, error: profileError } = await supabase
      .from("profile")
      .select("*")
      .eq("id", userId)
      .single();

    if (profileError || !userProfile) {
      throw new Error(`User profile not found for ${userId}`);
    }

    const initialHandicapIndex =
      userProfile.initialHandicapIndex !== undefined &&
      userProfile.initialHandicapIndex !== null
        ? Number(userProfile.initialHandicapIndex)
        : MAX_SCORE_DIFFERENTIAL;

    // 2. Fetch all approved rounds for user
    const { data: userRoundsRaw, error: roundsError } = await supabase
      .from("round")
      .select("*")
      .eq("userId", userId)
      .eq("approvalStatus", "approved")
      .order("teeTime", { ascending: true });

    if (roundsError) {
      throw roundsError;
    }

    const parsedRounds = roundResponseSchema.safeParse(userRoundsRaw);
    if (!parsedRounds.success) {
      throw new Error("Invalid rounds data: " + parsedRounds.error.message);
    }

    const userRounds = parsedRounds.data;

    // If no approved rounds, set handicap to maximum using stored procedure
    if (!userRounds.length) {
      const { error: rpcError } = await supabase.rpc(
        "process_handicap_no_rounds",
        {
          user_id: userId,
          max_handicap: MAX_SCORE_DIFFERENTIAL,
          queue_job_id: job.id,
        }
      );

      if (rpcError) {
        throw rpcError;
      }

      console.log(`No approved rounds for user ${userId}, handicap set to max`);
      return;
    }

    // 3. Fetch tee info
    const teeIds = new Set(userRounds.map((r) => r.teeId));
    const { data: teesRaw, error: teesError } = await supabase
      .from("teeInfo")
      .select("*")
      .in("id", Array.from(teeIds));

    if (teesError) throw teesError;

    const parsedTees = teeResponseSchema.safeParse(teesRaw);
    if (!parsedTees.success) {
      throw new Error("Invalid tees data: " + parsedTees.error.message);
    }
    const tees = parsedTees.data;

    // 4. Fetch holes
    const { data: holesRaw, error: holesError } = await supabase
      .from("hole")
      .select("*")
      .in("teeId", Array.from(teeIds));

    if (holesError) throw holesError;

    const parsedHoles = holeResponseSchema.safeParse(holesRaw);
    if (!parsedHoles.success) {
      throw new Error("Invalid holes data: " + parsedHoles.error.message);
    }
    const holes = parsedHoles.data;

    // 5. Fetch scores
    const roundIds = userRounds.map((r) => r.id);
    const { data: scoresRaw, error: scoresError } = await supabase
      .from("score")
      .select("*")
      .in("roundId", roundIds);

    if (scoresError) throw scoresError;

    const parsedScores = scoreResponseSchema.safeParse(scoresRaw);
    if (!parsedScores.success) {
      throw new Error("Invalid scores data: " + parsedScores.error.message);
    }
    const scores = parsedScores.data;

    // Build in-memory maps
    const teeMap = new Map(tees.map((tee) => [tee.id, tee]));
    const roundScoresMap = new Map(
      roundIds.map((roundId) => [
        roundId,
        scores.filter((s) => s.roundId === roundId),
      ])
    );
    const holesMap = new Map(
      Array.from(teeIds).map((teeId) => [
        teeId,
        holes.filter((h) => h.teeId === teeId),
      ])
    );

    // Initialize processed rounds array
    const processedRounds: ProcessedRound[] = userRounds.map((r) => ({
      id: r.id,
      teeTime: new Date(r.teeTime),
      existingHandicapIndex: MAX_SCORE_DIFFERENTIAL,
      rawDifferential: 0,
      esrOffset: 0,
      finalDifferential: 0,
      updatedHandicapIndex: 0,
      adjustedGrossScore: 0,
      adjustedPlayedScore: 0,
      teeId: r.teeId,
      courseHandicap: 0,
      approvalStatus: r.approvalStatus,
    }));

    // Pass 1: Calculate adjusted gross scores
    for (const pr of processedRounds) {
      const teePlayed = teeMap.get(pr.teeId);
      if (!teePlayed) throw new Error(`Tee not found for round ${pr.id}`);

      const roundScores = roundScoresMap.get(pr.id);
      if (!roundScores) throw new Error(`Scores not found for round ${pr.id}`);

      const holes = holesMap.get(pr.teeId);
      if (!holes) throw new Error(`Holes not found for tee ${pr.teeId}`);

      const numberOfHolesPlayed = roundScores.length;

      const courseHandicap = calculateCourseHandicap(
        pr.existingHandicapIndex,
        teePlayed,
        numberOfHolesPlayed
      );

      const scoresWithHcpStrokes = addHcpStrokesToScores(
        holes,
        roundScores,
        courseHandicap,
        numberOfHolesPlayed
      );

      const adjustedPlayedScore = calculateAdjustedPlayedScore(
        holes,
        scoresWithHcpStrokes
      );

      const adjustedGrossScore = calculateAdjustedGrossScore(
        adjustedPlayedScore,
        courseHandicap,
        numberOfHolesPlayed,
        holes,
        roundScores
      );

      pr.adjustedGrossScore = adjustedGrossScore;
      pr.adjustedPlayedScore = adjustedPlayedScore;
      pr.courseHandicap = courseHandicap;
    }

    // Pass 2: Calculate raw differentials and detect ESR
    let rollingIndex = initialHandicapIndex;
    for (let i = 0; i < processedRounds.length; i++) {
      const pr = processedRounds[i];
      pr.existingHandicapIndex = rollingIndex;

      const teePlayed = teeMap.get(pr.teeId);
      if (!teePlayed) throw new Error(`Tee not found for round ${pr.id}`);

      pr.rawDifferential = calculateScoreDifferential(
        pr.adjustedGrossScore,
        teePlayed.courseRating18,
        teePlayed.slopeRating18
      );

      const startIdx = Math.max(0, i - (ESR_WINDOW_SIZE - 1));
      const relevantDifferentials = processedRounds
        .slice(startIdx, i + 1)
        .map((round) => round.rawDifferential);
      pr.updatedHandicapIndex = calculateHandicapIndex(relevantDifferentials);

      const difference = rollingIndex - pr.rawDifferential;
      if (difference >= EXCEPTIONAL_ROUND_THRESHOLD) {
        const offset = difference >= 10 ? 2 : 1;
        const startIdx = Math.max(
          0,
          i - (Math.min(ESR_WINDOW_SIZE, i + 1) - 1)
        );
        for (let j = startIdx; j <= i; j++) {
          processedRounds[j].esrOffset += offset;
        }
      }

      rollingIndex = pr.updatedHandicapIndex;
    }

    // Pass 3: Apply ESR offsets and calculate final differentials
    for (let i = 0; i < processedRounds.length; i++) {
      const pr = processedRounds[i];
      pr.existingHandicapIndex =
        i === 0
          ? initialHandicapIndex
          : processedRounds[i - 1].updatedHandicapIndex;

      pr.finalDifferential = pr.rawDifferential - pr.esrOffset;
      const startIdx = Math.max(0, i - (ESR_WINDOW_SIZE - 1));
      const relevantDifferentials = processedRounds
        .slice(startIdx, i + 1)
        .map((r) => r.finalDifferential);
      const calculatedIndex = calculateHandicapIndex(relevantDifferentials);

      if (processedRounds.length >= 20) {
        const lowHandicapIndex = calculateLowHandicapIndex(processedRounds, i);
        pr.updatedHandicapIndex = applyHandicapCaps(
          calculatedIndex,
          lowHandicapIndex
        );
      } else {
        pr.updatedHandicapIndex = calculatedIndex;
      }

      pr.updatedHandicapIndex = Math.min(
        pr.updatedHandicapIndex,
        MAX_SCORE_DIFFERENTIAL
      );
    }

    // Perform all DB updates atomically in a single transaction via stored procedure
    const roundUpdates = processedRounds.map((pr) => ({
      id: pr.id,
      existingHandicapIndex: pr.existingHandicapIndex,
      scoreDifferential: pr.finalDifferential,
      updatedHandicapIndex: pr.updatedHandicapIndex,
      exceptionalScoreAdjustment: pr.esrOffset,
      adjustedGrossScore: pr.adjustedGrossScore,
      courseHandicap: pr.courseHandicap,
      adjustedPlayedScore: pr.adjustedPlayedScore,
    }));

    const { data: rpcResult, error: rpcError } = await supabase.rpc(
      "process_handicap_updates",
      {
        round_updates: roundUpdates,
        user_id: userId,
        new_handicap_index:
          processedRounds[processedRounds.length - 1].updatedHandicapIndex,
        queue_job_id: job.id,
      }
    );

    if (rpcError) {
      throw rpcError;
    }

    console.log(`Successfully processed handicap for user ${userId}`);
  } catch (error: unknown) {
    // Handle failure: update queue entry with error
    console.error(`Failed to process user ${job.user_id}:`, error);

    const errorMessage =
      error instanceof Error ? error.message : "Unknown error";
    const newAttempts = job.attempts + 1;
    const newStatus = newAttempts >= MAX_RETRIES ? "failed" : "pending";

    try {
      await supabase
        .from("handicap_calculation_queue")
        .update({
          attempts: newAttempts,
          error_message: errorMessage,
          status: newStatus,
          last_updated: new Date().toISOString(),
        })
        .eq("id", job.id);
    } catch (updateError) {
      console.error(
        `Failed to update error status for job ${job.id} in handicap_calculation_queue:`,
        updateError
      );
    }

    // Re-throw so Promise.allSettled marks as rejected
    throw error;
  }
}
```

### Success Criteria

#### Automated Verification:
- [x] TypeScript compilation passes: `pnpm build`
- [ ] No linting errors: `pnpm lint`
- [ ] Route accessible locally: `curl -H "Authorization: Bearer <secret>" http://localhost:3000/api/cron/process-handicap-queue`

#### Manual Verification:
- [ ] Route returns 403 without correct auth header
- [ ] Route returns 200 with correct auth header
- [ ] Can process pending queue jobs manually via curl
- [ ] Handicap calculations are identical to edge function results
- [ ] Failed jobs increment retry counter correctly
- [ ] Jobs deleted from queue after successful processing

---

## Phase 4: Configure Vercel Cron Job

### Overview

Add the new handicap processor to Vercel's cron configuration to run every minute.

### Changes Required

#### 1. Update Vercel Configuration

**File**: `vercel.json`
**Changes**: Add handicap processor to crons array

```json
{
  "crons": [
    {
      "path": "/api/cron/reconcile-stripe",
      "schedule": "0 2 * * *"
    },
    {
      "path": "/api/cron/process-handicap-queue",
      "schedule": "* * * * *"
    }
  ]
}
```

#### 2. Deploy to Vercel

**Manual steps:**
1. Commit changes: `git add . && git commit -m "Add Vercel cron for handicap processing"`
2. Push to repository: `git push`
3. Verify deployment in Vercel dashboard
4. Check cron job is scheduled in Vercel → Settings → Cron Jobs

### Success Criteria

#### Automated Verification:
- [x] `vercel.json` is valid JSON
- [ ] Build succeeds on Vercel
- [ ] Deployment completes successfully

#### Manual Verification:
- [ ] Cron job appears in Vercel dashboard under "Cron Jobs"
- [ ] Cron job shows correct schedule (`* * * * *`)
- [ ] Cron job shows correct path (`/api/cron/process-handicap-queue`)
- [ ] Wait 1 minute and check Vercel logs for execution
- [ ] Verify handicap calculations run successfully in production

---

## Phase 5: Disable Supabase pg_cron System

### Overview

Comment out the pg_cron setup to prevent conflicts while keeping the database trigger and queue table intact. This allows easy rollback if needed.

### Changes Required

#### 1. Create Migration to Disable Cron

**File**: `supabase/migrations/20251217000000_disable_handicap_cron.sql` (new file)
```sql
-- Disable Supabase pg_cron for handicap queue processing
-- The queue and trigger remain active, but processing is now handled by Vercel cron
-- This migration can be reverted if needed by calling setup_handicap_queue_cron()

-- Remove the cron job
SELECT cron.unschedule('process-handicap-queue');

-- Keep the setup function for documentation/rollback purposes
-- Keep the queue table and trigger (still used by Vercel cron)

COMMENT ON FUNCTION setup_handicap_queue_cron() IS
  'DISABLED: Handicap queue processing migrated to Vercel cron.
   This function remains for rollback purposes only.';
```

#### 2. Update Supabase Config

**File**: `supabase/config.toml`
**Changes**: Remove edge function configuration

```toml
# Remove these lines:
[functions.process-handicap-queue]
verify_jwt = false  # Allow cron job to call without authentication
```

#### 3. Apply Migration

**Commands:**
```bash
# Local
supabase db reset

# Production (via Supabase dashboard or CLI)
supabase db push
```

### Success Criteria

#### Automated Verification:
- [ ] Migration applies without errors: `supabase db reset`
- [ ] No SQL syntax errors in migration file

#### Manual Verification:
- [ ] Check `cron.job` table: cron job removed
- [ ] Queue table still exists and functional
- [ ] Trigger still fires on round changes
- [ ] Vercel cron continues to process queue
- [ ] No duplicate processing (both systems not running)
- [ ] Monitor for 24 hours to ensure stability

---

## Phase 6: Remove Handicap Edge Function

### Overview

Completely remove the Supabase Edge Function now that Vercel cron is proven to work. This is the final cleanup step.

### Changes Required

#### 1. Delete Edge Function Directory

**Files to delete:**
- `supabase/functions/process-handicap-queue/index.ts`
- `supabase/functions/process-handicap-queue/deno.json`
- `supabase/functions/process-handicap-queue/` (entire directory)

**DO NOT DELETE:**
- `supabase/functions/handicap-shared/` (keep as reference, or delete in separate cleanup)
- `supabase/functions/send-verification-email/` (different edge function)

#### 2. Update Documentation

**File**: Create `supabase/functions/README.md` (new file)
```markdown
# Supabase Edge Functions

## Active Functions

### send-verification-email
Custom email sending hook for Supabase Auth.

## Removed Functions

### process-handicap-queue (Removed 2024-12-17)
Migrated to Vercel cron at `app/api/cron/process-handicap-queue/route.ts`.
See migration plan: `.claude/plans/migrate-handicap-cron-to-vercel/251217.md`

## Shared Utilities

### handicap-shared (Deprecated)
Handicap calculation utilities migrated to `lib/handicap/`.
This directory can be removed after verification period.
```

### Success Criteria

#### Automated Verification:
- [ ] Edge function directory deleted
- [ ] Build still succeeds: `pnpm build`
- [ ] No broken imports or references

#### Manual Verification:
- [ ] Handicap processing continues to work via Vercel cron
- [ ] No errors in production logs
- [ ] Queue continues to drain properly
- [ ] All handicap calculations remain accurate

---

## Testing Strategy

### Unit Tests (Future Enhancement)

While not included in this migration, consider adding:
- `lib/handicap/calculations.test.ts` - Test all calculation functions
- `app/api/cron/process-handicap-queue/route.test.ts` - Test API route logic

**Key test cases:**
- Handicap index calculation with different round counts
- ESR detection and application
- Soft/hard cap logic
- Edge cases (0 rounds, exactly 20 rounds, etc.)
- Queue retry logic
- Authentication failure scenarios

### Integration Tests

**Manual test scenarios:**

1. **Happy Path:**
   - Submit a new round
   - Verify queue entry created
   - Wait 1 minute
   - Verify handicap updated
   - Verify queue entry deleted

2. **Multiple Users:**
   - Submit rounds for 30+ users
   - Verify batch processing (25 at a time)
   - Verify all eventually processed
   - Verify no duplicate processing

3. **Error Handling:**
   - Insert invalid data in queue
   - Verify error logged
   - Verify retry counter incremented
   - Verify marked failed after 3 attempts

4. **Zero Rounds:**
   - User with no approved rounds
   - Verify handicap set to 54.0
   - Verify queue entry deleted

5. **Database Reset:**
   - Run `supabase db reset`
   - Verify system still works
   - Verify no manual steps required

### Performance Testing

**Metrics to monitor:**
- Processing time for batch of 25 users
- Memory usage during processing
- Database query count (should match edge function)
- Cron execution time in Vercel logs

**Expected performance:**
- <10 seconds for batch of 25 users
- <500MB memory usage
- ~15-20 database queries per user

### Monitoring

**Add logging for:**
- Queue size before/after processing
- Processing time per user
- Success/failure counts
- Retry attempts
- Any errors or exceptions

**Check Vercel logs:**
- Cron execution frequency (should be every minute)
- Success rate (should be >99%)
- Error messages
- Processing times

---

## Performance Considerations

### Network Latency

**Vercel → Supabase:** ~20-100ms additional latency vs Edge Function
**Impact:** Negligible - database queries take 2-5 seconds total
**Mitigation:** Not needed for background job

### Cold Starts

**Vercel serverless:** ~100-500ms cold start
**Frequency:** Low (cron keeps function warm)
**Impact:** Minimal - only affects first execution after idle period

### Batch Size

**Current:** 25 users per minute
**Capacity:** 36,000 users per day (25 × 60 × 24)
**Recommendation:** Monitor queue depth; increase batch size if needed

### Database Connections

**Pattern:** One connection per cron execution
**Duration:** <10 seconds typical
**Concern:** Connection pooling handled by Supabase

---

## Migration Notes

### Rollback Plan

If issues arise, rollback is straightforward:

1. **Immediate rollback:**
   - Remove Vercel cron from `vercel.json`
   - Re-enable pg_cron: `SELECT setup_handicap_queue_cron();`
   - Edge function still exists (if Phase 6 not completed)

2. **After edge function deleted:**
   - Revert to previous commit
   - Redeploy edge function
   - Re-enable pg_cron

### Data Safety

**No data loss risk:**
- Queue table unchanged
- Trigger unchanged
- RPC functions unchanged
- Both systems can coexist temporarily

### Deployment Strategy

**Recommended approach:**
1. Deploy Vercel cron (Phases 1-4)
2. Run both systems in parallel for 24 hours
3. Verify identical results
4. Disable pg_cron (Phase 5)
5. Monitor for 1 week
6. Remove edge function (Phase 6)

### Local Development

**Before migration:**
- Required manual vault setup after `db reset`
- Cron worked but authentication failed

**After migration:**
- `supabase db reset` works seamlessly
- No manual steps required
- Cron secret in `.env.local`

---

## References

- Original feedback: Security review comment about `body.scheduled` vulnerability
- Existing pattern: `app/api/cron/reconcile-stripe/route.ts`
- Edge function: `supabase/functions/process-handicap-queue/index.ts`
- Handicap utilities: `supabase/functions/handicap-shared/utils.ts`
- Queue table: `supabase/migrations/20251207150151_exotic_sabra.sql`
- Trigger: `supabase/migrations/20251207150152_replace_handicap_trigger.sql`
- RPC functions: `supabase/migrations/20251207213412_add_process_handicap_updates_function.sql`
